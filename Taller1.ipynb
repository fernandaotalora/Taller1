{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 1 Buscador"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Authors: \n",
    "Fernanda Otalora 865100607\n",
    "Yurany Cortes    865100603     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan inicialmente todos los archivos que tienen las noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Abrir el archivo inicial y tomar el documento html.\n",
    "\n",
    "archivo = open(\"archivosinicial/reut2-000.sgm\", \"r\")\n",
    "soup1  = BeautifulSoup(archivo, 'html.parser')\n",
    "archivo.close()\n",
    "\n",
    "documentos = []\n",
    "documentos = soup1.find_all('reuters')\n",
    "\n",
    "fileInitial = open(\"archivoprocesado/reut2-000.txt\", \"w\")\n",
    "fileRaw = open(\"archivoprocesado/raw.txt\", \"w\")\n",
    "\n",
    "for i in range(len(documentos)):\n",
    "        try:          \n",
    "            cadena = documentos[i].title.string.replace('\\n',' ')+\" \"+documentos[i].body.string.replace('\\n',' ')\n",
    "            cadena = cadena.lower()\n",
    "            cadena = cadena.replace(' reuter','')           \n",
    "            fileInitial.write(cadena+\"\\n\") \n",
    "            cadena=documentos[i].title.string + \"@@,\"+ documentos[i].body.string +\"@@;\" \n",
    "            fileRaw.write(cadena+\"\\n\") \n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "fileInitial.close()\n",
    "fileRaw.close()\n",
    "\n",
    "words = []\n",
    "stopwords = open(\"archivosinicial/stopwords.txt\", \"r\")\n",
    "dato = stopwords.readline()\n",
    "words = dato.split(';')\n",
    "stopwords.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la funcion en la cual se aplican expresiones regulares para limpiar los documentos recibidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDocument(documentos,archivoProcesado):\n",
    "    for cadena in documentos.readlines():\n",
    "        try:                      \n",
    "            cadena = re.sub(r'<.*>|[0-9]|[,*$]|[.*$]|[-*$]|[(.*)$]|[/*$]|[\"*$]|[\\'][a-z|\\W]|[+*$]|[:*$]',\" \",cadena)        \n",
    "            for j in range(len(words)): \n",
    "                cadena = re.sub(\" \" + words[j]+\" \",\" \",cadena)\n",
    "                \n",
    "            archivoProcesado.write(cadena)                   \n",
    "        except:\n",
    "            pass        \n",
    "    \n",
    "\n",
    "archivoProcesado = open(\"archivoprocesado/fileDocumentos.txt\", \"w\")\n",
    "documentos =  open(\"archivosinicial/reut2-000.txt\", \"r\")\n",
    "processDocument(documentos,archivoProcesado)\n",
    "archivoProcesado.close()\n",
    "documentos.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear el diccionario de los documentos recibidos.\n",
    "Se define un patron para hacer split en lo que no sea caracteres alfanumericos y se obtiene el diccionario de palabras y se aplica la funcion set para quitar duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def getDictionary(fileDoc,fileDic):\n",
    "    pattern = re.compile(r'\\W+')\n",
    "\n",
    "    fileWords = open(\"archivoprocesado/\"+fileDoc, \"r\")\n",
    "    dictionary = pattern.split(fileWords.read())\n",
    "    \n",
    "    dictionary = sorted(list(set(dictionary)))\n",
    "   \n",
    "    dictionary.remove(\"\")\n",
    "    fileWords.close()\n",
    "    \n",
    "    dicString = str(dictionary).lstrip(\"[\").rstrip(\"]\").replace(\", \",\",\")\n",
    "    dicArray  = dicString.split(\",\")\n",
    "    dicFinal = \"\"\n",
    "    \n",
    "    for i in range(len(dicArray)):\n",
    "        dicFinal = dicFinal+\",\"+dicArray[i].lstrip(\"'\").rstrip(\"'\")\n",
    "    \n",
    "    \n",
    "    #Guardar el diccionar en un archivo.\n",
    "    fileDictionary = open(\"archivoprocesado/\"+fileDic, \"w\")\n",
    "    fileDictionary.write(dicFinal.lstrip(\",\"))\n",
    "    fileDictionary.close()\n",
    "    \n",
    "getDictionary(\"fileDocumentos.txt\",\"dictionary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que crea la matrizTF y la guarda en un archivo, cuenta la cantidad de palabras en cada documento para crear el vector con los denominadores para obtener la matrizTFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def setMatrizTF(fileMatriz,fileDic,fileDoc):\n",
    "    \n",
    "    dictionary  = open('archivoprocesado/'+fileDic).read()    \n",
    "    dictionary  = dictionary.split(\",\")\n",
    "    \n",
    "    #Crear el Vector Space Model\n",
    "    vectorSpace = open(\"archivoprocesado/\"+fileMatriz, \"w\")\n",
    "    fileFinally = open(\"archivoprocesado/\"+fileDoc, \"r\")\n",
    "\n",
    "    w=len(dictionary)\n",
    "    h=len(fileFinally.readlines())\n",
    "\n",
    "    n=0\n",
    "\n",
    "    denominador=[0 for x in range(w)]\n",
    "\n",
    "    linea=\"\"\n",
    "\n",
    "    fileFinally = open(\"archivoprocesado/\"+fileDoc, \"r\")\n",
    "    for lines in fileFinally.readlines(): \n",
    "        lineaAux=\"\"\n",
    "        for k in range(len(dictionary)):                                                 \n",
    "            patron = re.compile(r''+dictionary[k]+'')        \n",
    "            count = len(patron.findall(lines))\n",
    "\n",
    "            lineaAux = lineaAux+\",\"+str(count)\n",
    "\n",
    "            if count > 0:            \n",
    "                denominador[k]=denominador[k]+1\n",
    "            else:\n",
    "                denominador[k]=denominador[k]+0\n",
    "\n",
    "        lineaAux = lineaAux.lstrip(\",\")\n",
    "        linea = linea+lineaAux +\"\\n\"\n",
    "        n=n+1    \n",
    "\n",
    "    linea = linea.rstrip(\"\\n\")\n",
    "    vectorSpace.write(linea)\n",
    "    vectorSpace.close()\n",
    "    fileFinally.close()  \n",
    "    \n",
    "    fileDen = open(\"archivoprocesado/denominador.txt\", \"w\")\n",
    "    fileDen.write(str(denominador).lstrip(\"[\").rstrip(\"]\").replace(\", \",\",\"))\n",
    "    fileDen.close()\n",
    "    \n",
    "setMatrizTF(\"matriz_tf.txt\",\"dictionary.txt\",\"fileDocumentos.txt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que crea la matrizTFIDF, aplicando la formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimilitudCoseno(VectorArchivo, vectorBusqueda):\n",
    "    x = np.array(VectorArchivo)\n",
    "    y = np.array(vectorBusqueda)\n",
    "    dot = np.dot(x,y)\n",
    "    x_modulus = np.sqrt((x*x).sum())\n",
    "    y_modulus = np.sqrt((y*y).sum())\n",
    "    similitud=0\n",
    "    if(x_modulus != 0 and y_modulus != 0 ):\n",
    "        similitud = dot / x_modulus / y_modulus\n",
    "    return similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Tiempo de ejecucion 0.26602602005004883 -----------\n"
     ]
    }
   ],
   "source": [
    "# Obtener matriz TF-IDF\n",
    "import math\n",
    "import time\n",
    "\n",
    "InvertedIndex = []\n",
    "TFIDFDb=[]\n",
    "def getMatrizTFIDF():\n",
    "    den= open(\"archivoprocesado/denominador.txt\", \"r\").read()\n",
    "    denominador = den.split(\",\")\n",
    "\n",
    "    matrizTF = open('archivoprocesado/matriz_tf.txt').read()\n",
    "    matrizTF = [item.split() for item in matrizTF.split('\\n')]\n",
    "\n",
    "    w=len(denominador)\n",
    "    h=len(matrizTF)\n",
    "\n",
    "    #Creamos matriz TF-IDF\n",
    "    fileMatriz= open(\"archivoprocesado/matriz_tfidf.txt\", \"w\")\n",
    "\n",
    "    doc = []\n",
    "    \n",
    "    logTerm=[]\n",
    "    \n",
    "    for i in range(len(matrizTF)):\n",
    "        fila = matrizTF[i]  \n",
    "        doc  = str(fila).lstrip(\"['\").rstrip(\"']\").split(\",\")\n",
    "        nq = []\n",
    "        \n",
    "        for j in range(len(doc)): \n",
    "\n",
    "            if i==0:\n",
    "                InvertedIndex.append([])\n",
    "                logTerm.append([])\n",
    "                logTerm[j]=math.log(h/int(denominador[j]))\n",
    "                tfidf= int(doc[j])*logTerm[j]\n",
    "                if(tfidf>0):\n",
    "                    nq.append([j,tfidf])\n",
    "            else:\n",
    "                tfidf = int(doc[j])*logTerm[j]\n",
    "                if(tfidf>0):\n",
    "                    nq.append([j,tfidf])\n",
    "            if(tfidf>0):\n",
    "                InvertedIndex[j].append(i)\n",
    "                \n",
    "        TFIDFDb.append( {\"id\":i,\"tf_idf\":nq} )\n",
    "   \n",
    "    print(\" ------ Tiempo de ejecucion %s -----------\" % (time.time()-start_time))   \n",
    "\n",
    "\n",
    "    #Guardar matriz TF - IDF            \n",
    "\n",
    "       \n",
    "\n",
    "getMatrizTFIDF()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para tomar la informacion ingresada en la caja de texto, crear el vector query y hacer similitud de coseno con la matriz tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0.1468929125857383]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>\n",
       "TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Texas Commerce Bancshares Inc's Texas\n",
       "Commerce Bank-Houston said it filed an application with the\n",
       "Comptroller of the Currency in an effort to create the largest\n",
       "banking network in Harris County.\n",
       "    The bank said the network would link 31 banks having\n",
       "13.5 billion dlrs in assets and 7.5 billion dlrs in deposits. farmers\n",
       "       \n",
       " Reuter\n",
       "\u0003</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Tiempo de ejecucion 102.14594101905823 -----------\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#Generar la ventana que recibe el texto del buscador\n",
    "vent = Tk()\n",
    "\n",
    "a = StringVar()\n",
    "\n",
    "def vectorSearch():\n",
    "    pattern = re.compile(r'\\W+')\n",
    "    file  = open('archivoprocesado/fileSearch.txt').read()    \n",
    "    wordsSearch  = pattern.split(file)     \n",
    "     \n",
    "    dictionary  = open('archivoprocesado/dictionary.txt').read()    \n",
    "    dictionary  = dictionary.split(\",\")\n",
    "    \n",
    "    w=len(dictionary)\n",
    "    vector = []\n",
    "    vector = [0 for x in range(w)]\n",
    "    IdWords=[]\n",
    "    IdDocuments=[]\n",
    "    resultadosBusqueda=[]\n",
    "    for i in range(len(wordsSearch)):\n",
    "        try:\n",
    "            \n",
    "            ind = dictionary.index(wordsSearch[i])\n",
    "            IdWords.append(ind)\n",
    "            if(vector[ind]>0):\n",
    "                vector[ind] = vector[ind]+1\n",
    "            else:    \n",
    "                vector[ind] = 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if(len(IdWords)>0):\n",
    "        #Se busca con el indice invertido los documentos que tienen las palabras a buscar\n",
    "        for countWord in range(0,len(IdWords)):\n",
    "            for IdDocument in InvertedIndex[IdWords[countWord]]:\n",
    "                if(IdDocument not in IdDocuments):\n",
    "                    IdDocuments.append(IdDocument)\n",
    "    \n",
    "        countArchivos=0   \n",
    "        #Se recorre cada documento\n",
    "        for IdDocument in IdDocuments:\n",
    "                tf_idf =  [x for x in TFIDFDb if x['id'] == IdDocument][0]['tf_idf']\n",
    "                #armo la matriz TF-IDF\n",
    "                VectorArchivo = np.zeros(w)\n",
    "                for x in range(len(tf_idf)):\n",
    "                    VectorArchivo[tf_idf[x][0]]=tf_idf[x][1]\n",
    "                \n",
    "                resultadosBusqueda.append([])\n",
    "                #Calcula la similitud coseno de cada documento con el vector de busqueda\n",
    "                similitud = SimilitudCoseno(VectorArchivo, vector)\n",
    "\n",
    "                resultadosBusqueda[countArchivos].append(IdDocument)\n",
    "                resultadosBusqueda[countArchivos].append(similitud)\n",
    "                countArchivos+=1\n",
    "        resultadosBusqueda = sorted(resultadosBusqueda, key=lambda a_entry: a_entry[1],reverse=True) \n",
    "\n",
    "        #Se recuperan los documentos originales sin procesar\n",
    "        documentosRaw=open(\"archivoprocesado/raw.txt\", \"r\").read().split(\"@@;\")\n",
    "        clear_output()\n",
    "        #con los los resultados de la busqueda visualiza los documentos encontrados\n",
    "        for resultado in resultadosBusqueda:\n",
    "            print(resultado)\n",
    "            display(HTML('<h1>'+documentosRaw[resultado[0]].split(\"@@,\")[0]+'</h1>'))\n",
    "            display(HTML('<p>'+documentosRaw[resultado[0]].split(\"@@,\")[1]+'</p>'))\n",
    "    \n",
    "    \n",
    "    # vector tiene el query listo para aplicar similtud de coseno\n",
    "    #Leer la matriztf-idf para hacer similutd de coseno con el vector de query\n",
    "    \n",
    "    \n",
    "\n",
    "def valor():\n",
    "    a = entrada.get()\n",
    "    fileWords = open(\"archivoprocesado/buscador.txt\", \"w\")\n",
    "    a = a.lower()\n",
    "    fileWords.write(a)\n",
    "    fileWords.close()\n",
    "    \n",
    "    #Procesar las palabra ingresadas.\n",
    "    documentos=open(\"archivoprocesado/buscador.txt\", \"r\")\n",
    "    archivoProcesado = open(\"archivoprocesado/fileSearch.txt\", \"w\")\n",
    "    processDocument(documentos,archivoProcesado)\n",
    "    documentos.close()\n",
    "    archivoProcesado.close()\n",
    "    \n",
    "    vectorSearch()\n",
    "\n",
    "\n",
    "entrada = Entry(vent, width=30)\n",
    "entrada.grid(row=0, column=0)\n",
    "\n",
    "boton = Button(vent, text=\"Buscar\", command=valor)\n",
    "boton.grid(row=1, column=0)\n",
    "\n",
    "vent.mainloop()\n",
    "\n",
    "\n",
    "print(\" ------ Tiempo de ejecucion %s -----------\" % (time.time()-start_time))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
