{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto buscador de palabras con MongoDB Taller 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se lee el archivo inicial, se le quitan stopwords, caracteres especiales y se guarda en la base de datos en la coleccion documentos, la coleccion raw guarda los documentos como inicialmente se leen sin tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def conexionBD():\n",
    "    #Conexion a la base de datos\n",
    "    client = MongoClient()\n",
    "    client = pymongo.MongoClient(\"mongodb://testAdmin:12345@104.200.28.188:27017/buscador\")\n",
    "    db = client.buscador\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import re\n",
    "import bson\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup\n",
    "from Documentos import Documentos\n",
    "from collections import Counter\n",
    "from Diccionario import Diccionario\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "db = conexionBD()\n",
    "#Obtenemos las colecciones para trabajar y guardar la informacion\n",
    "collDocumentos   = db.documentos\n",
    "collRaw          = db.raw\n",
    "collDic          = db.diccionario\n",
    "collMatrizTF     = db.matriztf\n",
    "collDenominador  = db.denominador\n",
    "collLogTermin    = db.logtermin\n",
    "\n",
    "#Obtenemos los stopwords\n",
    "words = []\n",
    "stopwords = open(\"archivosinicial/stopwords.txt\", \"r\")\n",
    "dato = stopwords.readline()\n",
    "words = dato.split(';')\n",
    "stopwords.close() \n",
    "\n",
    "def tomarDocumentos():\n",
    "    \n",
    "    pattern = re.compile(r'\\W+')\n",
    "\n",
    "    #Preparamos los datos a guardar, se utiliza el ejemplo de archivos\n",
    "    archivo = open(\"archivosinicial/reut2-001.sgm\", \"r\")\n",
    "    soup1  = BeautifulSoup(archivo, 'html.parser')\n",
    "    archivo.close()\n",
    "\n",
    "    documentos = []\n",
    "    documentos = soup1.find_all('reuters')\n",
    "    \n",
    "    #Obtener la informacion de los archivos\n",
    "    jsonDocumento = [0 for x in range(len(documentos))]\n",
    "    jsonRaw       = [0 for x in range(len(documentos))]\n",
    "\n",
    "\n",
    "    for i in range(len(documentos)):\n",
    "        try:        \n",
    "            cadena = documentos[i].title.string.replace('\\n',' ')+\" \"+documentos[i].body.string.replace('\\n',' ')\n",
    "            cadena = cadena.lower()\n",
    "            cadena = cadena.replace(' reuter','')\n",
    "            cadena = re.sub(r'<.*>|[0-9]|[,*$]|[.*$]|[-*$]|[(.*)$]|[/*$]|[\"*$]|[\\'][a-z|\\W]|[+*$]|[:*$]',\" \",cadena)\n",
    "            for j in range(len(words)):             \n",
    "                cadena = re.sub(\" \" + words[j]+\" \",\" \",cadena)        \n",
    "            jsonDocumento[i] = Documentos(i,cadena)\n",
    "            cadena=documentos[i].title.string + \"@@\"+ documentos[i].body.string \n",
    "            jsonRaw[i] = Documentos(i,cadena)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Guardamos los objetos documentos en la coleccion inicial    \n",
    "    for doc in jsonDocumento:\n",
    "        collDocumentos.insert_one(doc.toDBCollection())\n",
    "\n",
    "    # Guardamos los objetos documentos en la coleccion raw    \n",
    "    for doc in jsonRaw:    \n",
    "        collRaw.insert_one(doc.toDBCollection())\n",
    "#tomarDocumentos()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda el diccionario en la base de datos en la coleccion diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDiccionario():\n",
    "    cadena =\"\"\n",
    "    cursor = collDocumentos.find()\n",
    "    for fut in cursor:\n",
    "        cadena = cadena+\" \"+fut['documento']\n",
    "    pattern = re.compile(r'\\W+')\n",
    "    dictionary = pattern.split(cadena)\n",
    "    dictionary = sorted(list(set(dictionary)))\n",
    "    dictionary.remove(\"\")    \n",
    "\n",
    "    #se guarda el diccionario en la coleccion diccionario.\n",
    "    for i in range(len(dictionary)):          \n",
    "        idDic = collDic.insert_one({\"_id\":i,\"word\":dictionary[i]}).inserted_id    \n",
    "\n",
    "    #collDic.delete_one({\"_id\":'+str(i)+'})\n",
    "#getDiccionario()    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Crear la matriztf y el denominador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def setMatrizTF():\n",
    "    \n",
    "    cursorDic = collDic.find()\n",
    "    longDic=cursorDic.count()\n",
    "\n",
    "    denominador=[0 for x in range(longDic)]\n",
    "    \n",
    "    cursorDoc = collDocumentos.find()    \n",
    "    for fut in cursorDoc:            \n",
    "        cursorDic = collDic.find()\n",
    "        for dic in cursorDic:                                                 \n",
    "            patron = re.compile(r''+dic['word']+'')        \n",
    "            count = len(patron.findall(fut['documento']))            \n",
    "            collMatrizTF.insert_one({\"idDoc\":fut['_id'],\"idword\":dic['_id'],\"cant\":count})\n",
    "            \n",
    "            if count > 0:                        \n",
    "                denominador[dic['_id']]=denominador[dic['_id']]+1\n",
    "            else:\n",
    "                denominador[dic['_id']]=denominador[dic['_id']]+0\n",
    "\n",
    "    \n",
    "    for i in range(len(denominador)):\n",
    "        collDenominador.insert_one({\"_id\":i,\"cantDocWord\":denominador[i]})        \n",
    "            \n",
    "#setMatrizTF()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Crear la matriz tfidf e indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Tiempo de ejecucion 0.34375476837158203 -----------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "collTFIDF = db.collectiontfidf\n",
    "collIndIn = db.collectionindinv\n",
    "\n",
    "\n",
    "def getMatrizTFIDF():\n",
    "    \n",
    "    #Obtenemos cursorDoc, para realizar el calculo\n",
    "    cursorDoc = collDocumentos.find()\n",
    "    cantDoc=cursorDoc.count()\n",
    "    \n",
    "    #Obtenemos tamaÃ±o del diccionario\n",
    "    cursorDic = collDic.find()\n",
    "    longDic=cursorDic.count()\n",
    "    \n",
    "    indiceInv = [0 for x in range(longDic)]\n",
    "    \n",
    "    cursorTF  = collMatrizTF.find()\n",
    "    for tf in cursorTF:     \n",
    "        cursorDen = collDenominador.find({\"_id\":{\"$in\":[tf['idword']]}})\n",
    "        for cur in cursorDen:                    \n",
    "            logTerm = math.log(cantDoc/cur['cantDocWord'])\n",
    "            tfidf = tf['cant']*logTerm           \n",
    "            collTFIDF.insert_one({\"idDoc\":tf['idDoc'],\"idWord\":tf['idword'],\"tfidf\":tfidf})\n",
    "                        \n",
    "            if(tf['cant']>0):    \n",
    "                indiceInv[tf['idword']] = str(indiceInv[tf['idword']]) +\"|\"+str(tf['idDoc'])\n",
    "                \n",
    "    \n",
    "    for i in range(len(indiceInv)):\n",
    "        docs = indiceInv[i].split(\"|\")                \n",
    "        documentos=[]\n",
    "        for j in range(len(docs)):            \n",
    "            if(j>0):\n",
    "                documentos.append(docs[j])\n",
    "        collIndIn.insert_one({\"idword\":i,\"documentos\":[documentos]})\n",
    "#getMatrizTFIDF()\n",
    "\n",
    "#print(\"Indice Invertido\")\n",
    "#col2 = collIndIn.find(\n",
    "#for cur2 in  col2:\n",
    "    #print(cur2)\n",
    "#print(\"Tfidf\")\n",
    "#col2 = collTFIDF.find()\n",
    "#for cur2 in  col2:\n",
    "#   print(cur2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "print(\" ------ Tiempo de ejecucion %s -----------\" % (time.time()-start_time))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion de similitud de coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SimilitudCoseno(VectorArchivo, vectorBusqueda):\n",
    "    x = np.array(VectorArchivo)\n",
    "    y = np.array(vectorBusqueda)\n",
    "    dot = np.dot(x,y)\n",
    "    x_modulus = np.sqrt((x*x).sum())\n",
    "    y_modulus = np.sqrt((y*y).sum())\n",
    "    similitud=0\n",
    "    if(x_modulus != 0 and y_modulus != 0 ):\n",
    "        similitud = dot / x_modulus / y_modulus\n",
    "    return similitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir la funcion para realizar la busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buscar(cadena):\n",
    "    # se imprime el diccionario\n",
    "    dic = db.diccionario.find()\n",
    "    for cur2 in  dic:\n",
    "         print(cur2)\n",
    "\n",
    "    cadena = cadena.lower()\n",
    "    cadena = re.sub(r'<.*>|[0-9]|[,*$]|[.*$]|[-*$]|[(.*)$]|[/*$]|[\"*$]|[\\'][a-z|\\W]|[+*$]|[:*$]',\" \",cadena)\n",
    "    for j in range(len(words)):             \n",
    "        cadena = re.sub(\" \" + words[j]+\" \",\" \",cadena)     \n",
    "   \n",
    "    resultadosBusqueda= []\n",
    "    listIdWords=[]\n",
    "    NumTerms=db.diccionario.count()\n",
    "    #cantidad de documentos\n",
    "    cursorDoc = collDocumentos.find()\n",
    "    cantDoc=cursorDoc.count()\n",
    "    idDoc=[]\n",
    "    #Tomamos cada palabra ingresada en la busqueda\n",
    "    listaPalabras=Counter(cadena.split(\" \"))\n",
    "    vecBusqueda=np.zeros(NumTerms)\n",
    "    for WordF in listaPalabras.most_common():\n",
    "        num=WordF[1]  \n",
    "        findWord=db.diccionario.find_one({'word':WordF[0]})\n",
    "        if(findWord!= None):\n",
    "            IdWord=findWord['_id']\n",
    "            denom= collDenominador.find_one({\"_id\":{\"$in\":[IdWord]}})\n",
    "            valorDenom=denom['cantDocWord']\n",
    "            logTerm = math.log(cantDoc/valorDenom)\n",
    "            tfidfBusqueda=num*logTerm\n",
    "            vecBusqueda[IdWord]=tfidfBusqueda\n",
    "            if(num>0):\n",
    "                listIdWords.append(IdWord)\n",
    "    \n",
    "    tfidf=[]\n",
    "        \n",
    "    if(len(listIdWords)>0):\n",
    "        #Se busca con el indice invertido los documentos que tienen las palabras a buscar\n",
    "        IdDocuments= db.collectionindinv.find_one({'idword':listIdWords[0]})['documentos'][0]\n",
    "        for countWord in range(1,len(listIdWords)):\n",
    "            for IdDoc in db.collectionindinv.find_one({'idword':listIdWords[countWord]})['documentos'][0]:                   \n",
    "                if(IdDoc not in IdDocuments):\n",
    "                    IdDocuments.append(IdDoc)   \n",
    "     \n",
    "        countArchivos=0 \n",
    "        for IdDocument in IdDocuments:\n",
    "            VectorTfidf=[]\n",
    "            \n",
    "            for tfidfa in db.collectiontfidf.find({'idDoc':int(IdDocument)}):\n",
    "                \n",
    "                VectorTfidf.append(tfidfa['tfidf']) \n",
    "            resultadosBusqueda.append([])\n",
    "            similitud = SimilitudCoseno(VectorTfidf, vecBusqueda)\n",
    "           \n",
    "            resultadosBusqueda[countArchivos].append(IdDocument)\n",
    "            resultadosBusqueda[countArchivos].append(similitud)\n",
    "            countArchivos+=1\n",
    "        \n",
    "        resultadosBusqueda = sorted(resultadosBusqueda, key=lambda a_entry: a_entry[1],reverse=True)\n",
    "        clear_output()\n",
    "        for resultado in resultadosBusqueda:\n",
    "           \n",
    "            idDoc=resultado[0]\n",
    "            docdb= db.raw.find_one({\"_id\": int(idDoc)})\n",
    "           \n",
    "            doc=docdb['documento']\n",
    "            docVec=doc.split(\"@@\")\n",
    "           \n",
    "            display(HTML('<h1>'+docVec[0]+'</h1>'))\n",
    "            display(HTML('<p>'+docVec[1]+'</p>'))\n",
    "       \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>GE <GE> SAYS AMR <AMR> ORDER WORTH 650 MLN DLRS</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>General Electric Co said AMR\n",
       "Corp's oprder of GE CFG-80C2 engines to power 25 new <Airbus\n",
       "Industrie> A300-600R and 15 Boeing Co <BA> 767-300ER twinjets\n",
       "is worth over 650 mln dlrs.\n",
       "    The company said the order is the largest single one it has\n",
       "ever received for commercial aircraft engines.\n",
       "    AMR announced the order earlier today.\n",
       "    GE said deliveries will start in early 1988.\n",
       " Reuter\n",
       "\u0003</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>SANDOZ PLANS WEEDKILLER JOINT VENTURE IN USSR</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Sandoz AG said it planned a joint venture\n",
       "to produce herbicides in the Soviet Union.\n",
       "    The company said it had signed a letter of intent with the\n",
       "Soviet Ministry of Fertiliser Production to form the first\n",
       "foreign joint venture the ministry had undertaken since the\n",
       "Soviet Union allowed Western firms to enter into joint ventures\n",
       "two months ago.\n",
       "    The ministry and Sandoz will each have a 50 pct stake, but\n",
       "a company spokeswoman was unable to give details of the size of\n",
       "investment or planned output.\n",
       " Reuter\n",
       "\u0003</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>NATIONAL FSI INC <NFSI> 4TH QTR LOSS</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Shr loss six cts vs profit 19 cts\n",
       "    Net loss 166,000 vs profit 580,000\n",
       "    Revs 3,772,000 vs 5,545,000\n",
       "    Year\n",
       "    Shr loss 13 cts vs profit 52 cts\n",
       "    Net loss 391,000 vs profit 1,425,000\n",
       "    Revs 15.4 mln vs 16.6 mln\n",
       "    NOTE: 1985 year figures pro forma for purchase accounting\n",
       "adjustments resulting from March 1985 reeacquisition of company\n",
       "by its original shareholders before August 1985 initial public\n",
       "offering. \n",
       " Reuter\n",
       "\u0003</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#Generar la ventana que recibe el texto del buscador\n",
    "vent = Tk()\n",
    "a = StringVar()\n",
    "def valor():\n",
    "    a = entrada.get()    \n",
    "    buscar(a)\n",
    "\n",
    "entrada = Entry(vent, width=30)\n",
    "entrada.grid(row=0, column=0)\n",
    "\n",
    "boton = Button(vent, text=\"Buscar\", command=valor)\n",
    "boton.grid(row=1, column=0)\n",
    "\n",
    "vent.mainloop()\n",
    "\n",
    "\n",
    "print(\" ------ Tiempo de ejecucion %s -----------\" % (time.time()-start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
