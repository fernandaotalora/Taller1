{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 3 Matriz TF-IDF con libreria de python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Authors: \n",
    "Fernanda Otalora 865100607\n",
    "Yurany Cortes    865100603     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan inicialmente todos los archivos que tienen las noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 781)\t0.110055059818\n",
      "  (0, 181)\t0.0571078073447\n",
      "  (0, 977)\t0.0777010766464\n",
      "  (0, 436)\t0.0513588114904\n",
      "  (0, 711)\t0.0611223872337\n",
      "  (0, 483)\t0.057689941591\n",
      "  (0, 798)\t0.0594041041228\n",
      "  (0, 978)\t0.0628297785322\n",
      "  (0, 642)\t0.0939967402616\n",
      "  (0, 470)\t0.0520821472104\n",
      "  (0, 993)\t0.0495940341095\n",
      "  (0, 283)\t0.0467573238462\n",
      "  (0, 321)\t0.120446121489\n",
      "  (0, 515)\t0.0528521811711\n",
      "  (0, 934)\t0.128743264582\n",
      "  (0, 810)\t0.0659601476536\n",
      "  (0, 541)\t0.108845319432\n",
      "  (0, 270)\t0.0443847576385\n",
      "  (0, 413)\t0.060352353273\n",
      "  (0, 326)\t0.107350723404\n",
      "  (0, 598)\t0.0628297785322\n",
      "  (0, 207)\t0.329800738268\n",
      "  (0, 67)\t0.0477521819472\n",
      "  (0, 153)\t0.0589470384959\n",
      "  (0, 282)\t0.0438119411477\n",
      "  :\t:\n",
      "  (924, 158)\t0.193950739432\n",
      "  (924, 589)\t0.0894013871056\n",
      "  (924, 882)\t0.139088880906\n",
      "  (924, 88)\t0.0991623021178\n",
      "  (924, 560)\t0.261182133022\n",
      "  (924, 92)\t0.0833836628487\n",
      "  (924, 164)\t0.0537452721125\n",
      "  (924, 833)\t0.133765903827\n",
      "  (924, 394)\t0.103366478821\n",
      "  (924, 209)\t0.117207288348\n",
      "  (924, 595)\t0.0937500242606\n",
      "  (924, 435)\t0.0752723398454\n",
      "  (924, 805)\t0.0962917503782\n",
      "  (924, 128)\t0.0884353272727\n",
      "  (924, 925)\t0.0716618395415\n",
      "  (924, 471)\t0.0956285406656\n",
      "  (924, 534)\t0.116987131458\n",
      "  (924, 860)\t0.103366478821\n",
      "  (924, 865)\t0.120698384205\n",
      "  (924, 961)\t0.18871743292\n",
      "  (924, 996)\t0.09435871646\n",
      "  (924, 528)\t0.0914694723546\n",
      "  (924, 173)\t0.120698384205\n",
      "  (924, 732)\t0.307378747419\n",
      "  (924, 963)\t0.691073273795\n",
      " ------ Tiempo de ejecucion 1.594224214553833 -----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Abrir el archivo inicial y tomar el documento html.\n",
    "\n",
    "archivo = open(\"archivosinicial2/reut2-000.sgm\", \"r\")\n",
    "soup1  = BeautifulSoup(archivo, 'html.parser')\n",
    "\n",
    "archivo.close()\n",
    "documentos = []\n",
    "logTerm=[]\n",
    "documentos = soup1.find_all('reuters')\n",
    "#Arregla el archivo para quitar caracteres especiales, numeros, etc\n",
    "#para dejar los datos listos para el llamado a la libreria\n",
    "fileRaw = open(\"archivoprocesado2/raw.txt\", \"w\")\n",
    "logTerm=[]\n",
    "for i in range(len(documentos)):\n",
    "        try:          \n",
    "            cadena = documentos[i].title.string.replace('\\n',' ')+\" \"+documentos[i].body.string.replace('\\n',' ')\n",
    "           \n",
    "            cadena=documentos[i].title.string + \"@@,\"+ documentos[i].body.string +\"@@;\" \n",
    "            cadena = cadena.lower()  \n",
    "            cadena = re.sub(r'<.*>|[0-9]|[,*$]|[.*$]|[-*$]|[(.*)$]|[/*$]|[\"*$]|[\\'][a-z|\\W]|[+*$]|[:*$]',\" \",cadena) \n",
    "            cadena = cadena.replace(' reuter','') \n",
    "            cadena= cadena.replace('\\n','')\n",
    "            cadena=cadena.replace('@@;','');\n",
    "            cadena=cadena.replace('@@','');\n",
    "            cadena= cadena.replace('\\x03','')\n",
    "            logTerm.append(cadena)  \n",
    "            \n",
    "            fileRaw.write(cadena+\"\\n\")             \n",
    "        except:\n",
    "            pass   \n",
    "#calcula lingitud para ingreso a la funcion de tfidf\n",
    "nlen= len(logTerm)\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "#Vectoriza a partir del listado creadi con las noticias\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                             stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(logTerm[:nlen])\n",
    "filetfidf=open(\"archivoprocesado2/filetfidf.txt\", \"w\")\n",
    "print(tfidf) \n",
    "filetfidf.write(str(tfidf))\n",
    "fileRaw.close()\n",
    "filetfidf.close()\n",
    "print(\" ------ Tiempo de ejecucion %s -----------\" % (time.time()-start_time))   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with n_samples=925 and n_features=1000...\n",
      "done in 1.777s.\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
    "      % (nlen, n_features))\n",
    "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time.time()-start_time))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "vs net cts shr revs qtr mln avg shrs oper th mths jan note year sales profit nd st includes\n",
      "\n",
      "Topic #1:\n",
      "shares stock common said split share company offer offering outstanding stake holders exchange securities buy board group shareholders march tender\n",
      "\n",
      "Topic #2:\n",
      "cts march div record qtly pay prior vs dividend payout april sets quarterly mthly franklin payable corp insured raises free\n",
      "\n",
      "Topic #3:\n",
      "pct bond issue eurobond lead coupon manager issues march date priced payment franc year yen pays bonds february rate issuing\n",
      "\n",
      "Topic #4:\n",
      "oil tonnes prices opec said production market price output crude coffee report export talks ec industry government wheat quotas ico\n",
      "\n",
      "Topic #5:\n",
      "loss profit oper revs cts vs year shr qtr th note includes dlrs nil dec discontinued corp credit ago december\n",
      "\n",
      "Topic #6:\n",
      "billion january rose dlrs year february december francs pct surplus reserves rise fell deficit exports budget total government compared taiwan\n",
      "\n",
      "Topic #7:\n",
      "banks bank brazil debt said funaro credit week foreign commercial stg imf finance committee market bankers money banking central ongpin\n",
      "\n",
      "Topic #8:\n",
      "said company corp new unit contract agreement american products division acquisition financial venture group union agreed service based sell computer\n",
      "\n",
      "Topic #9:\n",
      "mln dlrs sales share stg quarter dlr gain year ended revenues sale extraordinary net loan reported earnings cash money tax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
